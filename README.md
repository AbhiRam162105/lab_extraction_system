# ğŸ§¬ Enterprise Lab Report Extraction System

[![Python 3.11+](https://img.shields.io/badge/python-3.11+-blue.svg)](https://www.python.org/downloads/)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.100+-green.svg)](https://fastapi.tiangolo.com/)
[![Docker](https://img.shields.io/badge/Docker-Ready-blue.svg)](https://www.docker.com/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

An enterprise-grade AI-powered system for extracting, standardizing, and managing lab report data from medical documents using Google's Gemini Vision API. Built for scale with async processing, intelligent caching, and PostgreSQL for production deployments.

---

## ğŸ“‹ Table of Contents

- [Features](#-features)
- [Architecture](#-architecture)
- [Quick Start](#-quick-start)
- [Project Structure](#-project-structure)
- [Configuration](#-configuration)
- [API Reference](#-api-reference)
- [Deployment](#-deployment)
- [Development](#-development)
- [License](#-license)

---

## âœ¨ Features

### ğŸ” 3-Pass Extraction Pipeline
| Pass | Function | Technology |
|------|----------|------------|
| **Pass 1** | Vision extraction with multi-prompt retry | Gemini Vision API |
| **Pass 2** | Structured JSON conversion with validation | Schema validation |
| **Pass 3** | Test name standardization with LOINC codes | Fuzzy matching + LLM |

### ğŸ¯ Intelligent Processing
- **Multi-prompt retry strategy** for difficult documents
- **Confidence-based validation** with automatic flagging for review
- **Enhanced image preprocessing** (deskewing, denoising, contrast enhancement)
- **Adaptive rate limiting** to prevent API throttling

### ğŸ“Š Test Name Standardization
- **100+ pre-mapped** common lab tests with LOINC codes
- **Fuzzy matching** (RapidFuzz) for alias recognition
- **Semantic matching** using sentence transformers
- **LLM fallback** for unknown tests

### âš¡ Performance Optimizations
- **Async batch processing** for high throughput
- **Two-tier caching** (Redis + disk) to reduce API calls
- **Connection pooling** for PostgreSQL
- **Horizontal scaling** with multiple workers

### ğŸ³ Production-Ready
- **Docker Compose** orchestration
- **PostgreSQL** for concurrent multi-writer safety
- **Redis** for job queuing and caching
- **Health checks** on all services
- **Graceful shutdown** handling

---

## ğŸ— Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         Streamlit Frontend                               â”‚
â”‚                        (http://localhost:8501)                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚ HTTP
                                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          FastAPI Backend                                 â”‚
â”‚                        (http://localhost:6000)                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚   Upload     â”‚  â”‚   Results    â”‚  â”‚        Queue Jobs              â”‚ â”‚
â”‚  â”‚   Endpoint   â”‚  â”‚   Endpoint   â”‚  â”‚     (Redis Queue)              â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â–¼                       â–¼                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  RQ Worker 1    â”‚    â”‚  RQ Worker 2    â”‚    â”‚  RQ Worker 3    â”‚
â”‚  (lab_reports)  â”‚    â”‚    (batch)      â”‚    â”‚ (high_priority) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                      â”‚                      â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚           3-Pass Extraction Pipeline          â”‚
         â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
         â”‚  â”‚ Preprocess â”‚â†’â”‚  Gemini    â”‚â†’â”‚ Standardizeâ”‚ â”‚
         â”‚  â”‚   Image    â”‚ â”‚  Vision    â”‚ â”‚   Tests   â”‚ â”‚
         â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â–¼                                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     PostgreSQL      â”‚                 â”‚       Redis         â”‚
â”‚   (lab_extraction)  â”‚                 â”‚  (Cache + Queue)    â”‚
â”‚    Port: 5432       â”‚                 â”‚    Port: 6379       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸš€ Quick Start

### Prerequisites

- **Docker** and **Docker Compose** (recommended)
- **Python 3.11+** (for local development)
- **Gemini API Key** from [Google AI Studio](https://aistudio.google.com/apikey)

### Option 1: Docker Compose (Recommended)

```bash
# 1. Clone the repository
git clone <repository-url>
cd lab_extraction_system

# 2. Configure environment
cp .env.example .env
# Edit .env and add your GEMINI__API_KEY

# 3. Start all services
docker-compose up --build

# 4. Access the application
# Frontend: http://localhost:8501
# API Docs: http://localhost:6000/docs
```

### Option 2: Production Deployment

```bash
# Use the optimized compose file with multiple workers
docker-compose -f docker-compose.optimized.yaml up --build
```

### Option 3: Local Development

```bash
# 1. Setup virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# 2. Install dependencies
pip install -r requirements.txt

# 3. Start Redis (required)
docker run -d -p 6379:6379 redis:7-alpine

# 4. Configure environment
cp .env.example .env
# Edit .env and add your GEMINI__API_KEY

# 5. Run the application
chmod +x start.sh
./start.sh
```

---

## ğŸ“ Project Structure

```
lab_extraction_system/
â”‚
â”œâ”€â”€ ğŸ“‚ backend/                    # FastAPI Backend Application
â”‚   â”œâ”€â”€ ğŸ“‚ api/                    # API route handlers
â”‚   â”œâ”€â”€ ğŸ“‚ core/                   # Core configuration
â”‚   â”‚   â”œâ”€â”€ config.py              # Settings management (Pydantic)
â”‚   â”‚   â”œâ”€â”€ database.py            # SQLAlchemy/SQLModel setup
â”‚   â”‚   â””â”€â”€ queue.py               # Redis queue connection
â”‚   â”œâ”€â”€ ğŸ“‚ models/                 # Database models
â”‚   â”‚   â””â”€â”€ document.py            # Document & Extraction models
â”‚   â”œâ”€â”€ ğŸ“‚ services/               # Business logic services
â”‚   â””â”€â”€ main.py                    # FastAPI application entry point
â”‚
â”œâ”€â”€ ğŸ“‚ workers/                    # Background Processing Workers
â”‚   â”œâ”€â”€ ğŸ“‚ extraction/             # Extraction pipeline
â”‚   â”‚   â”œâ”€â”€ __init__.py            # Package exports
â”‚   â”‚   â”œâ”€â”€ main.py                # Document processor entry
â”‚   â”‚   â”œâ”€â”€ gemini.py              # 3-pass Gemini extraction pipeline
â”‚   â”‚   â”œâ”€â”€ preprocessing.py       # Image preprocessing (deskew, denoise)
â”‚   â”‚   â”œâ”€â”€ fast_preprocessing.py  # Optimized parallel preprocessing
â”‚   â”‚   â”œâ”€â”€ prompts.py             # Multi-prompt extraction strategies
â”‚   â”‚   â”œâ”€â”€ standardizer.py        # Test name standardization
â”‚   â”‚   â”œâ”€â”€ semantic_matcher.py    # Sentence transformer matching
â”‚   â”‚   â”œâ”€â”€ batch_processor.py     # Async batch processing
â”‚   â”‚   â”œâ”€â”€ cache_manager.py       # Two-tier caching (Redis + disk)
â”‚   â”‚   â”œâ”€â”€ rate_limiter.py        # Adaptive rate limiting
â”‚   â”‚   â””â”€â”€ optimized_worker.py    # Enhanced RQ worker
â”‚   â”œâ”€â”€ ğŸ“‚ queue/                  # Queue management
â”‚   â””â”€â”€ ğŸ“‚ tasks/                  # Task definitions
â”‚
â”œâ”€â”€ ğŸ“‚ frontend_app/               # Streamlit Frontend
â”‚   â”œâ”€â”€ main.py                    # Main dashboard UI
â”‚   â””â”€â”€ ğŸ“‚ pages/                  # Multi-page app
â”‚       â””â”€â”€ 1_ğŸ“Š_Global_Tests.py   # Global test analysis page
â”‚
â”œâ”€â”€ ğŸ“‚ config/                     # Configuration Files
â”‚   â”œâ”€â”€ settings.yaml              # Application settings
â”‚   â””â”€â”€ test_mappings.yaml         # Lab test â†’ LOINC mappings (100+)
â”‚
â”œâ”€â”€ ğŸ“‚ storage/                    # File Storage
â”‚   â””â”€â”€ ğŸ“‚ lab-reports/            # Uploaded lab report images
â”‚
â”œâ”€â”€ ğŸ“‚ scripts/                    # Utility scripts
â”‚
â”œâ”€â”€ ğŸ³ Docker Files
â”‚   â”œâ”€â”€ docker-compose.yaml        # Standard deployment
â”‚   â”œâ”€â”€ docker-compose.optimized.yaml  # Production with 3 workers
â”‚   â”œâ”€â”€ Dockerfile.backend         # Backend image
â”‚   â”œâ”€â”€ Dockerfile.worker          # Worker image
â”‚   â””â”€â”€ Dockerfile.frontend        # Frontend image
â”‚
â”œâ”€â”€ ğŸ“„ Configuration
â”‚   â”œâ”€â”€ .env.example               # Environment template
â”‚   â”œâ”€â”€ .env                       # Local environment (gitignored)
â”‚   â”œâ”€â”€ requirements.txt           # Python dependencies
â”‚   â””â”€â”€ .gitignore                 # Git ignore rules
â”‚
â”œâ”€â”€ ğŸ“„ Documentation
â”‚   â””â”€â”€ README.md                  # This file
â”‚
â””â”€â”€ ğŸ”§ Scripts
    â”œâ”€â”€ start.sh                   # Local development startup
    â””â”€â”€ wait_for_service.py        # Service readiness checker
```

---

## âš™ï¸ Configuration

### Environment Variables

All configuration is managed via environment variables. Copy `.env.example` to `.env` and configure:

#### Required Settings

| Variable | Description | Example |
|----------|-------------|---------|
| `GEMINI__API_KEY` | Your Gemini API key | `AIza...` |

#### Core Settings

| Variable | Description | Default |
|----------|-------------|---------|
| `GEMINI__MODEL` | Gemini model to use | `gemini-2.5-flash-lite` |
| `GEMINI__RATE_LIMIT` | Requests per minute | `15` |
| `DATABASE__URL` | Database connection string | PostgreSQL for Docker |
| `REDIS__URL` | Redis connection string | `redis://redis:6379/0` |

#### Processing Settings

| Variable | Description | Default |
|----------|-------------|---------|
| `PROCESSING__BATCH_SIZE` | Images per batch | `15` |
| `PROCESSING__MAX_RETRIES` | Retry attempts | `3` |
| `PROCESSING__ENABLE_CACHING` | Enable result caching | `true` |
| `PROCESSING__CACHE_TTL_HOURS` | Cache duration | `24` |

#### Standardization Settings

| Variable | Description | Default |
|----------|-------------|---------|
| `STANDARDIZATION__FUZZY_THRESHOLD` | Minimum fuzzy match score | `0.85` |
| `STANDARDIZATION__LLM_FALLBACK` | Use LLM for unknown tests | `true` |

---

## ğŸ“¡ API Reference

### Base URL
- **Development**: `http://localhost:6000/api/v1`
- **Docker**: `http://backend:6000/api/v1`

### Endpoints

| Method | Endpoint | Description |
|--------|----------|-------------|
| `POST` | `/upload` | Upload documents for processing |
| `GET` | `/documents` | List all documents |
| `GET` | `/results/{id}` | Get extraction results |
| `GET` | `/tasks/{id}` | Check task status |
| `GET` | `/cache/stats` | Get cache statistics |
| `POST` | `/batch/upload` | Batch upload multiple files |

### Example: Upload Document

```bash
curl -X POST "http://localhost:6000/api/v1/upload" \
  -F "files=@lab_report.pdf"
```

### Example: Get Results

```bash
curl "http://localhost:6000/api/v1/results/{document_id}"
```

### Response Schema

```json
{
  "lab_results": [
    {
      "test_name": "Hemoglobin",
      "original_name": "Hb",
      "value": "14.5",
      "unit": "g/dL",
      "reference_range": "12.0-16.0",
      "category": "Hematology",
      "loinc_code": "718-7",
      "standardization": {
        "is_standardized": true,
        "confidence": 1.0,
        "match_type": "exact"
      }
    }
  ],
  "patient_info": {
    "name": "John Doe",
    "patient_id": "12345",
    "age": "45",
    "gender": "M"
  },
  "metadata": {
    "confidence_score": 0.92,
    "needs_review": false,
    "total_tests_extracted": 15,
    "standardization": {
      "standardized_count": 14,
      "standardization_rate": 0.93
    }
  }
}
```

---

## ğŸ³ Deployment

### Standard Deployment

```bash
docker-compose up --build -d
```

### Production Deployment (Multiple Workers)

```bash
docker-compose -f docker-compose.optimized.yaml up --build -d
```

### Service Health Check

```bash
# Check all services
docker-compose ps

# View logs
docker-compose logs -f

# Check specific service
docker-compose logs backend
```

### Scaling Workers

```bash
# Scale workers for higher throughput
docker-compose up --scale worker=3 -d
```

---

## ğŸ”§ Development

### Adding New Test Mappings

Edit `config/test_mappings.yaml`:

```yaml
mappings:
  hemoglobin:
    canonical_name: "Hemoglobin"
    loinc_code: "718-7"
    category: "Hematology"
    unit: "g/dL"
    reference_range: "12.0-16.0"
    aliases:
      - "hb"
      - "hgb"
      - "haemoglobin"
```

### Running Tests

```bash
# Run backend tests
pytest backend/tests/

# Run with coverage
pytest --cov=backend --cov=workers
```

### Code Style

```bash
# Format code
black backend/ workers/ frontend_app/

# Lint
flake8 backend/ workers/
```

---

## ğŸ“Š Monitoring

### Access Points

| Service | URL | Description |
|---------|-----|-------------|
| Frontend | http://localhost:8501 | Streamlit Dashboard |
| API Docs | http://localhost:6000/docs | Swagger UI |
| ReDoc | http://localhost:6000/redoc | Alternative API docs |
| PostgreSQL | localhost:5432 | Database |
| Redis | localhost:6379 | Cache & Queue |

### Docker Health Status

All services include health checks:
- **PostgreSQL**: `pg_isready`
- **Redis**: `redis-cli ping`
- **Backend**: `curl /docs`
- **Frontend**: `curl /_stcore/health`
- **Workers**: Redis connectivity check

---

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

---

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ™ Acknowledgments

- [Google Gemini](https://deepmind.google/technologies/gemini/) for Vision AI
- [FastAPI](https://fastapi.tiangolo.com/) for the backend framework
- [Streamlit](https://streamlit.io/) for the frontend
- [RapidFuzz](https://github.com/maxbachmann/RapidFuzz) for fuzzy matching
- [LOINC](https://loinc.org/) for medical test standardization

---

<p align="center">
  Made with â¤ï¸ for healthcare data interoperability
</p>
